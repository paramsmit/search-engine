---
swagger: "2.0"
info: !php/object "O:8:\"stdClass\":11:{s:7:\"contact\";O:8:\"stdClass\":2:{s:4:\"name\";s:6:\"Google\";s:3:\"url\";s:18:\"https://google.com\";}s:11:\"description\";s:66:\"Converts
  audio to text by applying powerful neural network models.\";s:5:\"title\";s:12:\"Cloud
  Speech\";s:7:\"version\";s:7:\"v1beta1\";s:23:\"x-apiClientRegistration\";O:8:\"stdClass\":1:{s:3:\"url\";s:37:\"https://console.developers.google.com\";}s:21:\"x-apisguru-categories\";a:1:{i:0;s:16:\"machine_learning\";}s:6:\"x-logo\";O:8:\"stdClass\":1:{s:3:\"url\";s:116:\"https://api.apis.guru/v2/cache/logo/https_www.google.com_images_branding_googlelogo_2x_googlelogo_color_272x92dp.png\";}s:8:\"x-origin\";a:1:{i:0;O:8:\"stdClass\":4:{s:9:\"converter\";O:8:\"stdClass\":2:{s:3:\"url\";s:45:\"https://github.com/lucybot/api-spec-converter\";s:7:\"version\";s:5:\"2.6.2\";}s:6:\"format\";s:6:\"google\";s:3:\"url\";s:61:\"https://speech.googleapis.com/$discovery/rest?version=v1beta1\";s:7:\"version\";s:2:\"v1\";}}s:11:\"x-preferred\";b:0;s:14:\"x-providerName\";s:14:\"googleapis.com\";s:13:\"x-serviceName\";s:6:\"speech\";}"
host: speech.googleapis.com
basePath: /
paths:
  /v1beta1/operations/{name}:
    get:
      description: Gets the latest state of a long-running operation.  Clients can
        use thisnmethod to poll the operation result at intervals as recommended by
        the APInservice.
      operationId: speech.operations.get
      parameters:
      - description: The name of the operation resource.
        in: path
        name: name
        required: true
        type: string
        x-reservedExpansion: true
      responses:
        200:
          description: Successful response
          schema:
            $ref: '#/definitions/Operation'
      security:
      - Oauth2:
        - https://www.googleapis.com/auth/cloud-platform
      tags:
      - operations
    parameters:
    - $ref: '#/parameters/$.xgafv'
    - $ref: '#/parameters/access_token'
    - $ref: '#/parameters/alt'
    - $ref: '#/parameters/bearer_token'
    - $ref: '#/parameters/callback'
    - $ref: '#/parameters/fields'
    - $ref: '#/parameters/key'
    - $ref: '#/parameters/oauth_token'
    - $ref: '#/parameters/pp'
    - $ref: '#/parameters/prettyPrint'
    - $ref: '#/parameters/quotaUser'
    - $ref: '#/parameters/uploadType'
    - $ref: '#/parameters/upload_protocol'
  /v1beta1/speech:asyncrecognize:
    parameters:
    - $ref: '#/parameters/$.xgafv'
    - $ref: '#/parameters/access_token'
    - $ref: '#/parameters/alt'
    - $ref: '#/parameters/bearer_token'
    - $ref: '#/parameters/callback'
    - $ref: '#/parameters/fields'
    - $ref: '#/parameters/key'
    - $ref: '#/parameters/oauth_token'
    - $ref: '#/parameters/pp'
    - $ref: '#/parameters/prettyPrint'
    - $ref: '#/parameters/quotaUser'
    - $ref: '#/parameters/uploadType'
    - $ref: '#/parameters/upload_protocol'
    post:
      description: 'Performs asynchronous speech recognition: receive results via
        then[google.longrunning.Operations]n(/speech/reference/rest/v1beta1/operations#Operation)ninterface.
        Returns either ann`Operation.error` or an `Operation.response` which containsnan
        `AsyncRecognizeResponse` message.'
      operationId: speech.speech.asyncrecognize
      parameters:
      - in: body
        name: body
        schema:
          $ref: '#/definitions/AsyncRecognizeRequest'
      responses:
        200:
          description: Successful response
          schema:
            $ref: '#/definitions/Operation'
      security:
      - Oauth2:
        - https://www.googleapis.com/auth/cloud-platform
      tags:
      - speech
  /v1beta1/speech:syncrecognize:
    parameters:
    - $ref: '#/parameters/$.xgafv'
    - $ref: '#/parameters/access_token'
    - $ref: '#/parameters/alt'
    - $ref: '#/parameters/bearer_token'
    - $ref: '#/parameters/callback'
    - $ref: '#/parameters/fields'
    - $ref: '#/parameters/key'
    - $ref: '#/parameters/oauth_token'
    - $ref: '#/parameters/pp'
    - $ref: '#/parameters/prettyPrint'
    - $ref: '#/parameters/quotaUser'
    - $ref: '#/parameters/uploadType'
    - $ref: '#/parameters/upload_protocol'
    post:
      description: 'Performs synchronous speech recognition: receive results after
        all audionhas been sent and processed.'
      operationId: speech.speech.syncrecognize
      parameters:
      - in: body
        name: body
        schema:
          $ref: '#/definitions/SyncRecognizeRequest'
      responses:
        200:
          description: Successful response
          schema:
            $ref: '#/definitions/SyncRecognizeResponse'
      security:
      - Oauth2:
        - https://www.googleapis.com/auth/cloud-platform
      tags:
      - speech
schemes:
- https
definitions: !php/object "O:8:\"stdClass\":10:{s:21:\"AsyncRecognizeRequest\";O:8:\"stdClass\":3:{s:11:\"description\";s:73:\"The
  top-level message sent by the client for the `AsyncRecognize` method.\";s:10:\"properties\";O:8:\"stdClass\":2:{s:5:\"audio\";O:8:\"stdClass\":2:{s:4:\"$ref\";s:30:\"#/definitions/RecognitionAudio\";s:11:\"description\";s:43:\"*Required*
  The audio data to be recognized.\";}s:6:\"config\";O:8:\"stdClass\":2:{s:4:\"$ref\";s:31:\"#/definitions/RecognitionConfig\";s:11:\"description\";s:92:\"*Required*
  Provides information to the recognizer that specifies how to\nprocess the request.\";}}s:4:\"type\";s:6:\"object\";}s:9:\"Operation\";O:8:\"stdClass\":3:{s:11:\"description\";s:91:\"This
  resource represents a long-running operation that is the result of a\nnetwork API
  call.\";s:10:\"properties\";O:8:\"stdClass\":5:{s:4:\"done\";O:8:\"stdClass\":2:{s:11:\"description\";s:155:\"If
  the value is `false`, it means the operation is still in progress.\nIf `true`, the
  operation is completed, and either `error` or `response` is\navailable.\";s:4:\"type\";s:7:\"boolean\";}s:5:\"error\";O:8:\"stdClass\":2:{s:4:\"$ref\";s:20:\"#/definitions/Status\";s:11:\"description\";s:69:\"The
  error result of the operation in case of failure or cancellation.\";}s:8:\"metadata\";O:8:\"stdClass\":3:{s:20:\"additionalProperties\";O:8:\"stdClass\":1:{s:11:\"description\";s:61:\"Properties
  of the object. Contains field @type with type URL.\";}s:11:\"description\";s:281:\"Service-specific
  metadata associated with the operation.  It typically\ncontains progress information
  and common metadata such as create time.\nSome services might not provide such metadata.
  \ Any method that returns a\nlong-running operation should document the metadata
  type, if any.\";s:4:\"type\";s:6:\"object\";}s:4:\"name\";O:8:\"stdClass\":2:{s:11:\"description\";s:203:\"The
  server-assigned name, which is only unique within the same service that\noriginally
  returns it. If you use the default HTTP mapping, the\n`name` should have the format
  of `operations/some/unique/name`.\";s:4:\"type\";s:6:\"string\";}s:8:\"response\";O:8:\"stdClass\":3:{s:20:\"additionalProperties\";O:8:\"stdClass\":1:{s:11:\"description\";s:61:\"Properties
  of the object. Contains field @type with type URL.\";}s:11:\"description\";s:492:\"The
  normal response of the operation in case of success.  If the original\nmethod returns
  no data on success, such as `Delete`, the response is\n`google.protobuf.Empty`.
  \ If the original method is standard\n`Get`/`Create`/`Update`, the response should
  be the resource.  For other\nmethods, the response should have the type `XxxResponse`,
  where `Xxx`\nis the original method name.  For example, if the original method name\nis
  `TakeSnapshot()`, the inferred response type is\n`TakeSnapshotResponse`.\";s:4:\"type\";s:6:\"object\";}}s:4:\"type\";s:6:\"object\";}s:16:\"RecognitionAudio\";O:8:\"stdClass\":3:{s:11:\"description\";s:253:\"Contains
  audio data in the encoding specified in the `RecognitionConfig`.\nEither `content`
  or `uri` must be supplied. Supplying both or neither\nreturns google.rpc.Code.INVALID_ARGUMENT.
  See\n[audio limits](https://cloud.google.com/speech/limits#content).\";s:10:\"properties\";O:8:\"stdClass\":2:{s:7:\"content\";O:8:\"stdClass\":3:{s:11:\"description\";s:185:\"The
  audio data bytes encoded as specified in\n`RecognitionConfig`. Note: as with all
  bytes fields, protobuffers use a\npure binary representation, whereas JSON representations
  use base64.\";s:6:\"format\";s:4:\"byte\";s:4:\"type\";s:6:\"string\";}s:3:\"uri\";O:8:\"stdClass\":2:{s:11:\"description\";s:387:\"URI
  that points to a file that contains audio data bytes as specified in\n`RecognitionConfig`.
  Currently, only Google Cloud Storage URIs are\nsupported, which must be specified
  in the following format:\n`gs://bucket_name/object_name` (other URI formats return\ngoogle.rpc.Code.INVALID_ARGUMENT).
  For more information, see\n[Request URIs](https://cloud.google.com/storage/docs/reference-uris).\";s:4:\"type\";s:6:\"string\";}}s:4:\"type\";s:6:\"object\";}s:17:\"RecognitionConfig\";O:8:\"stdClass\":3:{s:11:\"description\";s:81:\"Provides
  information to the recognizer that specifies how to process the\nrequest.\";s:10:\"properties\";O:8:\"stdClass\":6:{s:8:\"encoding\";O:8:\"stdClass\":3:{s:11:\"description\";s:74:\"*Required*
  Encoding of audio data sent in all `RecognitionAudio` messages.\";s:4:\"enum\";a:6:{i:0;s:20:\"ENCODING_UNSPECIFIED\";i:1;s:8:\"LINEAR16\";i:2;s:4:\"FLAC\";i:3;s:5:\"MULAW\";i:4;s:3:\"AMR\";i:5;s:6:\"AMR_WB\";}s:4:\"type\";s:6:\"string\";}s:12:\"languageCode\";O:8:\"stdClass\":2:{s:11:\"description\";s:292:\"*Optional*
  The language of the supplied audio as a BCP-47 language tag.\nExample: \"en-GB\"
  \ https://www.rfc-editor.org/rfc/bcp/bcp47.txt\nIf omitted, defaults to \"en-US\".
  See\n[Language Support](https://cloud.google.com/speech/docs/languages)\nfor a list
  of the currently supported language codes.\";s:4:\"type\";s:6:\"string\";}s:15:\"maxAlternatives\";O:8:\"stdClass\":3:{s:11:\"description\";s:356:\"*Optional*
  Maximum number of recognition hypotheses to be returned.\nSpecifically, the maximum
  number of `SpeechRecognitionAlternative` messages\nwithin each `SpeechRecognitionResult`.\nThe
  server may return fewer than `max_alternatives`.\nValid values are `0`-`30`. A value
  of `0` or `1` will return a maximum of\none. If omitted, will return a maximum of
  one.\";s:6:\"format\";s:5:\"int32\";s:4:\"type\";s:7:\"integer\";}s:15:\"profanityFilter\";O:8:\"stdClass\":2:{s:11:\"description\";s:235:\"*Optional*
  If set to `true`, the server will attempt to filter out\nprofanities, replacing
  all but the initial character in each filtered word\nwith asterisks, e.g. \"f***\".
  If set to `false` or omitted, profanities\nwon't be filtered out.\";s:4:\"type\";s:7:\"boolean\";}s:10:\"sampleRate\";O:8:\"stdClass\":3:{s:11:\"description\";s:308:\"*Required*
  Sample rate in Hertz of the audio data sent in all\n`RecognitionAudio` messages.
  Valid values are: 8000-48000.\n16000 is optimal. For best results, set the sampling
  rate of the audio\nsource to 16000 Hz. If that's not possible, use the native sample
  rate of\nthe audio source (instead of re-sampling).\";s:6:\"format\";s:5:\"int32\";s:4:\"type\";s:7:\"integer\";}s:13:\"speechContext\";O:8:\"stdClass\":2:{s:4:\"$ref\";s:27:\"#/definitions/SpeechContext\";s:11:\"description\";s:71:\"*Optional*
  A means to provide context to assist the speech recognition.\";}}s:4:\"type\";s:6:\"object\";}s:13:\"SpeechContext\";O:8:\"stdClass\":3:{s:11:\"description\";s:93:\"Provides
  \"hints\" to the speech recognizer to favor specific words and phrases\nin the results.\";s:10:\"properties\";O:8:\"stdClass\":1:{s:7:\"phrases\";O:8:\"stdClass\":3:{s:11:\"description\";s:422:\"*Optional*
  A list of strings containing words and phrases \"hints\" so that\nthe speech recognition
  is more likely to recognize them. This can be used\nto improve the accuracy for
  specific words and phrases, for example, if\nspecific commands are typically spoken
  by the user. This can also be used\nto add additional words to the vocabulary of
  the recognizer. See\n[usage limits](https://cloud.google.com/speech/limits#content).\";s:5:\"items\";O:8:\"stdClass\":1:{s:4:\"type\";s:6:\"string\";}s:4:\"type\";s:5:\"array\";}}s:4:\"type\";s:6:\"object\";}s:28:\"SpeechRecognitionAlternative\";O:8:\"stdClass\":3:{s:11:\"description\";s:44:\"Alternative
  hypotheses (a.k.a. n-best list).\";s:10:\"properties\";O:8:\"stdClass\":2:{s:10:\"confidence\";O:8:\"stdClass\":3:{s:11:\"description\";s:457:\"*Output-only*
  The confidence estimate between 0.0 and 1.0. A higher number\nindicates an estimated
  greater likelihood that the recognized words are\ncorrect. This field is typically
  provided only for the top hypothesis, and\nonly for `is_final=true` results. Clients
  should not rely on the\n`confidence` field as it is not guaranteed to be accurate,
  or even set, in\nany of the results.\nThe default of 0.0 is a sentinel value indicating
  `confidence` was not set.\";s:6:\"format\";s:5:\"float\";s:4:\"type\";s:6:\"number\";}s:10:\"transcript\";O:8:\"stdClass\":2:{s:11:\"description\";s:73:\"*Output-only*
  Transcript text representing the words that the user spoke.\";s:4:\"type\";s:6:\"string\";}}s:4:\"type\";s:6:\"object\";}s:23:\"SpeechRecognitionResult\";O:8:\"stdClass\":3:{s:11:\"description\";s:68:\"A
  speech recognition result corresponding to a portion of the audio.\";s:10:\"properties\";O:8:\"stdClass\":1:{s:12:\"alternatives\";O:8:\"stdClass\":3:{s:11:\"description\";s:113:\"*Output-only*
  May contain one or more recognition hypotheses (up to the\nmaximum specified in
  `max_alternatives`).\";s:5:\"items\";O:8:\"stdClass\":1:{s:4:\"$ref\";s:42:\"#/definitions/SpeechRecognitionAlternative\";}s:4:\"type\";s:5:\"array\";}}s:4:\"type\";s:6:\"object\";}s:6:\"Status\";O:8:\"stdClass\":3:{s:11:\"description\";s:2437:\"The
  `Status` type defines a logical error model that is suitable for different\nprogramming
  environments, including REST APIs and RPC APIs. It is used by\n[gRPC](https://github.com/grpc).
  The error model is designed to be:\n\n- Simple to use and understand for most users\n-
  Flexible enough to meet unexpected needs\n\n# Overview\n\nThe `Status` message contains
  three pieces of data: error code, error message,\nand error details. The error code
  should be an enum value of\ngoogle.rpc.Code, but it may accept additional error
  codes if needed.  The\nerror message should be a developer-facing English message
  that helps\ndevelopers *understand* and *resolve* the error. If a localized user-facing\nerror
  message is needed, put the localized message in the error details or\nlocalize it
  in the client. The optional error details may contain arbitrary\ninformation about
  the error. There is a predefined set of error detail types\nin the package `google.rpc`
  that can be used for common error conditions.\n\n# Language mapping\n\nThe `Status`
  message is the logical representation of the error model, but it\nis not necessarily
  the actual wire format. When the `Status` message is\nexposed in different client
  libraries and different wire protocols, it can be\nmapped differently. For example,
  it will likely be mapped to some exceptions\nin Java, but more likely mapped to
  some error codes in C.\n\n# Other uses\n\nThe error model and the `Status` message
  can be used in a variety of\nenvironments, either with or without APIs, to provide
  a\nconsistent developer experience across different environments.\n\nExample uses
  of this error model include:\n\n- Partial errors. If a service needs to return partial
  errors to the client,\n    it may embed the `Status` in the normal response to indicate
  the partial\n    errors.\n\n- Workflow errors. A typical workflow has multiple steps.
  Each step may\n    have a `Status` message for error reporting.\n\n- Batch operations.
  If a client uses batch request and batch response, the\n    `Status` message should
  be used directly inside batch response, one for\n    each error sub-response.\n\n-
  Asynchronous operations. If an API call embeds asynchronous operation\n    results
  in its response, the status of those operations should be\n    represented directly
  using the `Status` message.\n\n- Logging. If some API errors are stored in logs,
  the message `Status` could\n    be used directly after any stripping needed for
  security/privacy reasons.\";s:10:\"properties\";O:8:\"stdClass\":3:{s:4:\"code\";O:8:\"stdClass\":3:{s:11:\"description\";s:66:\"The
  status code, which should be an enum value of google.rpc.Code.\";s:6:\"format\";s:5:\"int32\";s:4:\"type\";s:7:\"integer\";}s:7:\"details\";O:8:\"stdClass\":3:{s:11:\"description\";s:105:\"A
  list of messages that carry the error details.  There is a common set of\nmessage
  types for APIs to use.\";s:5:\"items\";O:8:\"stdClass\":2:{s:20:\"additionalProperties\";O:8:\"stdClass\":1:{s:11:\"description\";s:61:\"Properties
  of the object. Contains field @type with type URL.\";}s:4:\"type\";s:6:\"object\";}s:4:\"type\";s:5:\"array\";}s:7:\"message\";O:8:\"stdClass\":2:{s:11:\"description\";s:188:\"A
  developer-facing error message, which should be in English. Any\nuser-facing error
  message should be localized and sent in the\ngoogle.rpc.Status.details field, or
  localized by the client.\";s:4:\"type\";s:6:\"string\";}}s:4:\"type\";s:6:\"object\";}s:20:\"SyncRecognizeRequest\";O:8:\"stdClass\":3:{s:11:\"description\";s:72:\"The
  top-level message sent by the client for the `SyncRecognize` method.\";s:10:\"properties\";O:8:\"stdClass\":2:{s:5:\"audio\";O:8:\"stdClass\":2:{s:4:\"$ref\";s:30:\"#/definitions/RecognitionAudio\";s:11:\"description\";s:43:\"*Required*
  The audio data to be recognized.\";}s:6:\"config\";O:8:\"stdClass\":2:{s:4:\"$ref\";s:31:\"#/definitions/RecognitionConfig\";s:11:\"description\";s:92:\"*Required*
  Provides information to the recognizer that specifies how to\nprocess the request.\";}}s:4:\"type\";s:6:\"object\";}s:21:\"SyncRecognizeResponse\";O:8:\"stdClass\":3:{s:11:\"description\";s:153:\"The
  only message returned to the client by `SyncRecognize`. method. It\ncontains the
  result as zero or more sequential `SpeechRecognitionResult`\nmessages.\";s:10:\"properties\";O:8:\"stdClass\":1:{s:7:\"results\";O:8:\"stdClass\":3:{s:11:\"description\";s:101:\"*Output-only*
  Sequential list of transcription results corresponding to\nsequential portions of
  audio.\";s:5:\"items\";O:8:\"stdClass\":1:{s:4:\"$ref\";s:37:\"#/definitions/SpeechRecognitionResult\";}s:4:\"type\";s:5:\"array\";}}s:4:\"type\";s:6:\"object\";}}"
...
